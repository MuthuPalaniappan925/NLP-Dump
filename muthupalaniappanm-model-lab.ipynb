{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-10T04:11:19.956945Z","iopub.execute_input":"2024-04-10T04:11:19.957286Z","iopub.status.idle":"2024-04-10T04:11:20.767645Z","shell.execute_reply.started":"2024-04-10T04:11:19.957258Z","shell.execute_reply":"2024-04-10T04:11:20.766842Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Muthu Palaniappan M - 21011101079 - NLP Model Lab","metadata":{}},{"cell_type":"markdown","source":"## Installing Packages","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers\n!pip install -U datasets\n!pip install tensorboard\n!pip install sentencepiece\n!pip install accelerate\n!pip install evaluate\n!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:11:56.832917Z","iopub.execute_input":"2024-04-10T04:11:56.833818Z","iopub.status.idle":"2024-04-10T04:13:25.415246Z","shell.execute_reply.started":"2024-04-10T04:11:56.833785Z","shell.execute_reply":"2024-04-10T04:13:25.414227Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nCollecting responses<0.19 (from evaluate)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nInstalling collected packages: responses, evaluate\nSuccessfully installed evaluate-0.4.1 responses-0.18.0\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=91364340d93014222420d64ebd14e7340cd5eb53322d66f6c85a94ae365a17e9\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Importing Pacakges","metadata":{}},{"cell_type":"markdown","source":"- evaluate: The evaluate libraries helps us quickly evaluate transformer models from the Hugging Face library for different tasks. It can be text classification, question answering, and even text summarization.\n\n- rouge_score: Text summarization is primarily evaluated through Rouge score. To load the Rouge score metric code using the evaluate library, we need to install it although there isn’t any need to import it separately. We will get into the details of the Rouge score later in the article.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport pprint\nimport evaluate\nimport numpy as np\n \nfrom transformers import (\n    T5Tokenizer,\n    T5ForConditionalGeneration,\n    TrainingArguments,\n    Trainer\n)\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:13:25.417167Z","iopub.execute_input":"2024-04-10T04:13:25.417473Z","iopub.status.idle":"2024-04-10T04:13:43.293915Z","shell.execute_reply.started":"2024-04-10T04:13:25.417446Z","shell.execute_reply":"2024-04-10T04:13:43.292886Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-10 04:13:32.885680: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-10 04:13:32.885783: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-10 04:13:33.008835: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"markdown","source":"- There are 1779 samples in the training set and 445 samples in the validation set.\n- Using 80% of the samples for training and the rest for validation. The final training and validation splits are stored as dictionaries in dataset_train and dataset_valid","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset('gopalkalpande/bbc-news-summary', split='train')\nfull_dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\ndataset_train = full_dataset['train']\ndataset_valid = full_dataset['test']","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:14:59.168727Z","iopub.execute_input":"2024-04-10T04:14:59.169535Z","iopub.status.idle":"2024-04-10T04:15:02.198508Z","shell.execute_reply.started":"2024-04-10T04:14:59.169504Z","shell.execute_reply":"2024-04-10T04:15:02.197643Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5544e83362564ccbb6061059922f6e29"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 7.32M/7.32M [00:00<00:00, 12.9MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1580d17144fa4a23b80c920656c431a7"}},"metadata":{}}]},{"cell_type":"code","source":"print(dataset_train)\nprint(dataset_valid)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:15:08.131026Z","iopub.execute_input":"2024-04-10T04:15:08.131742Z","iopub.status.idle":"2024-04-10T04:15:08.136482Z","shell.execute_reply.started":"2024-04-10T04:15:08.131711Z","shell.execute_reply":"2024-04-10T04:15:08.135500Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['File_path', 'Articles', 'Summaries'],\n    num_rows: 1779\n})\nDataset({\n    features: ['File_path', 'Articles', 'Summaries'],\n    num_rows: 445\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset Analysis","metadata":{}},{"cell_type":"code","source":"def find_avg_sentence_length(dataset):\n    \"\"\"\n    Find the average sentence in the entire training set.\n    \"\"\"\n    sentence_lengths = []\n    for text in dataset:\n        corpus = [\n            word for word in text.split()\n        ]\n        sentence_lengths.append(len(corpus))\n    return sum(sentence_lengths)/len(sentence_lengths)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:23:59.642879Z","iopub.execute_input":"2024-04-10T04:23:59.643670Z","iopub.status.idle":"2024-04-10T04:23:59.649637Z","shell.execute_reply.started":"2024-04-10T04:23:59.643629Z","shell.execute_reply":"2024-04-10T04:23:59.648568Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"avg_article_length = find_avg_sentence_length(dataset_train['Articles'])\nprint(f\"Average article length: {avg_article_length} words\")\navg_summary_length = find_avg_sentence_length(dataset_train['Summaries'])\nprint(f\"Averrage summary length: {avg_summary_length} words\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:24:02.977522Z","iopub.execute_input":"2024-04-10T04:24:02.978497Z","iopub.status.idle":"2024-04-10T04:24:03.128015Z","shell.execute_reply.started":"2024-04-10T04:24:02.978459Z","shell.execute_reply":"2024-04-10T04:24:03.126937Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Average article length: 384.8555368184373 words\nAverrage summary length: 167.48341765036537 words\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- Nearly all summaries are below 200 words.\n- The average length of the articles is around 384 words.","metadata":{}},{"cell_type":"code","source":"def find_longest_length(dataset):\n    \"\"\"\n    Find the longest article and summary in the entire training set.\n    \"\"\"\n    max_length = 0\n    counter_4k = 0\n    counter_2k = 0\n    counter_1k = 0\n    counter_500 = 0\n    for text in dataset:\n        corpus = [\n            word for word in text.split()\n        ]\n        if len(corpus) > 4000:\n            counter_4k += 1\n        if len(corpus) > 2000:\n            counter_2k += 1\n        if len(corpus) > 1000:\n            counter_1k += 1\n        if len(corpus) > 500:\n            counter_500 += 1\n        if len(corpus) > max_length:\n            max_length = len(corpus)\n    return max_length, counter_4k, counter_2k, counter_1k, counter_500","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:25:03.675582Z","iopub.execute_input":"2024-04-10T04:25:03.676302Z","iopub.status.idle":"2024-04-10T04:25:03.683355Z","shell.execute_reply.started":"2024-04-10T04:25:03.676270Z","shell.execute_reply":"2024-04-10T04:25:03.682460Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"longest_article_length, counter_4k, counter_2k, counter_1k, counter_500 = find_longest_length(dataset_train['Articles'])\nprint(f\"Longest article length: {longest_article_length} words\")\nprint(f\"Artciles larger than 4000 words: {counter_4k}\")\nprint(f\"Artciles larger than 2000 words: {counter_2k}\")\nprint(f\"Artciles larger than 1000 words: {counter_1k}\")\nprint(f\"Artciles larger than 500 words: {counter_500}\")\nprint(\"------------------------------\")\nlongest_summary_length, counter_4k, counter_2k, counter_1k, counter_500 = find_longest_length(dataset_train['Summaries'])\nprint(f\"Longest summary length: {longest_summary_length} words\")\nprint(f\"Summaries larger than 4000 words: {counter_4k}\")\nprint(f\"Summaries larger than 2000 words: {counter_2k}\")\nprint(f\"Summaries larger than 1000 words: {counter_1k}\")\nprint(f\"Summaries larger than 500 words: {counter_500}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:25:22.619217Z","iopub.execute_input":"2024-04-10T04:25:22.619587Z","iopub.status.idle":"2024-04-10T04:25:22.763535Z","shell.execute_reply.started":"2024-04-10T04:25:22.619557Z","shell.execute_reply":"2024-04-10T04:25:22.762564Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Longest article length: 4377 words\nArtciles larger than 4000 words: 1\nArtciles larger than 2000 words: 7\nArtciles larger than 1000 words: 18\nArtciles larger than 500 words: 369\n------------------------------\nLongest summary length: 2073 words\nSummaries larger than 4000 words: 0\nSummaries larger than 2000 words: 1\nSummaries larger than 1000 words: 7\nSummaries larger than 500 words: 14\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- There is just one article above 4000 words and 356 articles above 500 words.","metadata":{}},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"MODEL = 't5-base'\nBATCH_SIZE = 4\nNUM_PROCS = 4\nEPOCHS = 2\nOUT_DIR = 'results_t5base'\nMAX_LENGTH = 512 # Maximum context length to consider while preparing dataset.","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:43:56.290213Z","iopub.execute_input":"2024-04-10T04:43:56.291076Z","iopub.status.idle":"2024-04-10T04:43:56.295607Z","shell.execute_reply.started":"2024-04-10T04:43:56.291043Z","shell.execute_reply":"2024-04-10T04:43:56.294749Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"- I choose to fine-tune the t5-base model. \n- The batch size is 4 and the number of processes used for parallel processing is 4 as well. \n- I will train for 2 epochs, and the maximum context length of the articles will be 512. \n- The average length of the articles is 384 words. Hence, any articles below 512 tokens will be padded, and any above 512 tokens will be truncated. \n- I feel this is the right size for this dataset.","metadata":{}},{"cell_type":"markdown","source":"## Tokenization\nTokenizing means converting a word into a numerical value. Sometimes a single word may be broken down into multiple ones.","metadata":{}},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(MODEL)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:28:43.066460Z","iopub.execute_input":"2024-04-10T04:28:43.067229Z","iopub.status.idle":"2024-04-10T04:28:44.360423Z","shell.execute_reply.started":"2024-04-10T04:28:43.067196Z","shell.execute_reply":"2024-04-10T04:28:44.359667Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4206a278e68405fb6e0ff59c4d781ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eca2a5cf53940ab83ad5d1ece467906"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfb1c65913f944889fa5d942deb4d651"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [f\"summarize: {article}\" for article in examples['Articles']]\n    model_inputs = tokenizer(\n        inputs,\n        max_length=MAX_LENGTH,\n        truncation=True,\n        padding='max_length'\n    )\n\n    # Set up the tokenizer for targets\n    targets = [summary for summary in examples['Summaries']]\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=MAX_LENGTH,\n            truncation=True,\n            padding='max_length'\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:28:56.291318Z","iopub.execute_input":"2024-04-10T04:28:56.292240Z","iopub.status.idle":"2024-04-10T04:28:56.298556Z","shell.execute_reply.started":"2024-04-10T04:28:56.292206Z","shell.execute_reply":"2024-04-10T04:28:56.297522Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tokenized_train = dataset_train.map(\n    preprocess_function,\n    batched=True,\n    num_proc=NUM_PROCS\n)\ntokenized_valid = dataset_valid.map(\n    preprocess_function,\n    batched=True,\n    num_proc=NUM_PROCS\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:29:04.288444Z","iopub.execute_input":"2024-04-10T04:29:04.288938Z","iopub.status.idle":"2024-04-10T04:29:09.573809Z","shell.execute_reply.started":"2024-04-10T04:29:04.288907Z","shell.execute_reply":"2024-04-10T04:29:09.572924Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1779 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430a3717e1d74c21ada046f5ad7330db"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/445 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8e0d46723cf45528551e292dfd1ea69"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model\n- I load the T5 Base model and move it to the computation device. \n- The T5 Base model contains around 223 million parameters. ","metadata":{}},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(MODEL)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:30:33.097839Z","iopub.execute_input":"2024-04-10T04:30:33.098247Z","iopub.status.idle":"2024-04-10T04:30:38.200624Z","shell.execute_reply.started":"2024-04-10T04:30:33.098212Z","shell.execute_reply":"2024-04-10T04:30:38.199647Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eeb7f48b91a4fd58e2799db2df4a9fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75adac815863443a9ae5f62a9729c923"}},"metadata":{}},{"name":"stdout","text":"222,903,552 total parameters.\n222,903,552 training parameters.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Score Metric: Rouge\nROUGE score is one of the most common metrics for evaluating deep learning based text summarization models.\n\n\n    ROUGE1: It is the ratio of the number of words that match the predictions and ground truth to the number of words in the predictions.\n    ROUGE2: It is the ratio of the number bi-grams that match in the predictions and the ground truth to the number of bi-grams in the predictions.\n    ROUGEL: It is a score defined by the longest matching sequence between the prediction and the ground truth.\n","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:31:39.432958Z","iopub.execute_input":"2024-04-10T04:31:39.433903Z","iopub.status.idle":"2024-04-10T04:31:40.891218Z","shell.execute_reply.started":"2024-04-10T04:31:39.433870Z","shell.execute_reply":"2024-04-10T04:31:40.890449Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee3d6a07c1ab4b94b22f8179334255d6"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred.predictions[0], eval_pred.label_ids\n\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    result = rouge.compute(\n        predictions=decoded_preds, \n        references=decoded_labels, \n        use_stemmer=True, \n        rouge_types=[\n            'rouge1', \n            'rouge2', \n            'rougeL'\n        ]\n    )\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:31:52.293838Z","iopub.execute_input":"2024-04-10T04:31:52.294233Z","iopub.status.idle":"2024-04-10T04:31:52.302166Z","shell.execute_reply.started":"2024-04-10T04:31:52.294205Z","shell.execute_reply":"2024-04-10T04:31:52.301038Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def preprocess_logits_for_metrics(logits, labels):\n    \"\"\"\n    Original Trainer may have a memory leak. \n    This is a workaround to avoid storing too many tensors that are not needed.\n    \"\"\"\n    pred_ids = torch.argmax(logits[0], dim=-1)\n    return pred_ids, labels","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:32:03.915501Z","iopub.execute_input":"2024-04-10T04:32:03.916167Z","iopub.status.idle":"2024-04-10T04:32:03.921218Z","shell.execute_reply.started":"2024-04-10T04:32:03.916133Z","shell.execute_reply":"2024-04-10T04:32:03.920115Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model","metadata":{}},{"cell_type":"code","source":"###Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=OUT_DIR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=OUT_DIR,\n    logging_steps=10,\n    evaluation_strategy='steps',\n    eval_steps=200,\n    save_strategy='epoch',\n    save_total_limit=2,\n    report_to='tensorboard',\n    learning_rate=0.0001,\n    dataloader_num_workers=4\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:44:04.968921Z","iopub.execute_input":"2024-04-10T04:44:04.969599Z","iopub.status.idle":"2024-04-10T04:44:04.975503Z","shell.execute_reply.started":"2024-04-10T04:44:04.969568Z","shell.execute_reply":"2024-04-10T04:44:04.974592Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_valid,\n    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:44:07.887993Z","iopub.execute_input":"2024-04-10T04:44:07.888357Z","iopub.status.idle":"2024-04-10T04:44:07.908511Z","shell.execute_reply.started":"2024-04-10T04:44:07.888331Z","shell.execute_reply":"2024-04-10T04:44:07.907582Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"history = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:44:10.559639Z","iopub.execute_input":"2024-04-10T04:44:10.560352Z","iopub.status.idle":"2024-04-10T05:01:03.599176Z","shell.execute_reply.started":"2024-04-10T04:44:10.560322Z","shell.execute_reply":"2024-04-10T05:01:03.598292Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='890' max='890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [890/890 16:51, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.404200</td>\n      <td>0.278079</td>\n      <td>0.918800</td>\n      <td>0.856900</td>\n      <td>0.904300</td>\n      <td>222.620200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.464500</td>\n      <td>0.278967</td>\n      <td>0.919800</td>\n      <td>0.857900</td>\n      <td>0.905000</td>\n      <td>222.622500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.331500</td>\n      <td>0.277035</td>\n      <td>0.919800</td>\n      <td>0.858000</td>\n      <td>0.905400</td>\n      <td>222.620200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.261400</td>\n      <td>0.267132</td>\n      <td>0.922000</td>\n      <td>0.861500</td>\n      <td>0.907800</td>\n      <td>222.620200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Saving Config","metadata":{}},{"cell_type":"code","source":"tokenizer.save_pretrained(OUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:02:16.432518Z","iopub.execute_input":"2024-04-10T05:02:16.432899Z","iopub.status.idle":"2024-04-10T05:02:16.445214Z","shell.execute_reply.started":"2024-04-10T05:02:16.432867Z","shell.execute_reply":"2024-04-10T05:02:16.444291Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"('results_t5base/tokenizer_config.json',\n 'results_t5base/special_tokens_map.json',\n 'results_t5base/spiece.model',\n 'results_t5base/added_tokens.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Post Processing","metadata":{}},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:03:19.272482Z","iopub.execute_input":"2024-04-10T05:03:19.272839Z","iopub.status.idle":"2024-04-10T05:03:19.277341Z","shell.execute_reply.started":"2024-04-10T05:03:19.272813Z","shell.execute_reply":"2024-04-10T05:03:19.276320Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model_path = f\"{OUT_DIR}/checkpoint-890\"  # the path where I saved my model\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\ntokenizer = T5Tokenizer.from_pretrained(OUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:03:57.451988Z","iopub.execute_input":"2024-04-10T05:03:57.452756Z","iopub.status.idle":"2024-04-10T05:03:58.274938Z","shell.execute_reply.started":"2024-04-10T05:03:57.452696Z","shell.execute_reply":"2024-04-10T05:03:58.273968Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"def summarize_text(text, model, tokenizer, max_length=512, num_beams=5):\n    ##Preprocess the text\n    inputs = tokenizer.encode(\n        \"summarize: \" + text,\n        return_tensors='pt',\n        max_length=max_length,\n        truncation=True\n    )\n\n    ##Generate the summary\n    summary_ids = model.generate(\n        inputs,\n        min_length=100,\n        max_length=101,\n        num_beams=num_beams,\n        # early_stopping=True,\n    )\n\n    ##Decode and return the summary\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:10:21.478933Z","iopub.execute_input":"2024-04-10T05:10:21.479297Z","iopub.status.idle":"2024-04-10T05:10:21.485416Z","shell.execute_reply.started":"2024-04-10T05:10:21.479268Z","shell.execute_reply":"2024-04-10T05:10:21.484455Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"dataset_train['Articles'][200]","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:18:30.909236Z","iopub.execute_input":"2024-04-10T05:18:30.910096Z","iopub.status.idle":"2024-04-10T05:18:30.939943Z","shell.execute_reply.started":"2024-04-10T05:18:30.910059Z","shell.execute_reply":"2024-04-10T05:18:30.938846Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"\"Smith loses US box office crown..New comedy Diary of a Mad Black Woman has ended Will Smith's reign at the top of the North American box office...Based on a play by Tyler Perry, who also stars as a gun-toting grandmother, the film took $22.7m (£11.8m) in its first three days of release. After topping the chart for two consecutive weeks, Smith's romantic comedy Hitch dropped to second place with takings of $21m (£10.9m). Keanu Reeves' supernatural thriller Constantine dropped a place to three. Based on the Hellblazer comics, the film took $11.8m (£6.1m) on its second week of release. Two new entries came next in the chart, with Wes Craven's horror movie Cursed, about a werewolf loose in Los Angeles, in fourth position with $9.5m (£4.9m)...Action comedy Man of the House, starring Tommy Lee Jones as a Texas ranger assigned to protect a cheerleader squad, came in at fifth with $9m (£4.6m). Clint Eastwood's boxing drama Million Dollar Baby - recipient of four Academy Awards, including best picture - continued to perform well in sixth place with takings of $7.2m (£3.74m). Martin Scorsese's Hollywood biopic The Aviator - which won five Oscars, all in minor categories - held on in ninth place. The low-budget feature Diary of a Mad Black Woman stars Kimberly Elise as a woman thrown out on the streets by her philandering husband. With the help of her grandmother Madea (one of three roles played by Perry), she plots revenge. Perry, 34, is one of America's best-known black playwrights but is a newcomer to film. Once made homeless after investing his own money in unsuccessful productions of his work, he now lives in the mansion in which Diary of a Mad Black Woman was filmed.\""},"metadata":{}}]},{"cell_type":"code","source":"text = dataset_train['Articles'][200]","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:28:58.084105Z","iopub.execute_input":"2024-04-10T05:28:58.084867Z","iopub.status.idle":"2024-04-10T05:28:58.109108Z","shell.execute_reply.started":"2024-04-10T05:28:58.084836Z","shell.execute_reply":"2024-04-10T05:28:58.108279Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"summary = summarize_text(text, model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:28:58.247435Z","iopub.execute_input":"2024-04-10T05:28:58.247812Z","iopub.status.idle":"2024-04-10T05:29:10.199519Z","shell.execute_reply.started":"2024-04-10T05:28:58.247783Z","shell.execute_reply":"2024-04-10T05:29:10.198708Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"## Downloading the weights","metadata":{}},{"cell_type":"code","source":"!zip -r {OUT_DIR} {OUT_DIR}","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:22:11.637459Z","iopub.execute_input":"2024-04-10T05:22:11.638232Z","iopub.status.idle":"2024-04-10T05:26:47.280814Z","shell.execute_reply.started":"2024-04-10T05:22:11.638202Z","shell.execute_reply":"2024-04-10T05:26:47.279592Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"  adding: results_t5base/ (stored 0%)\n  adding: results_t5base/spiece.model (deflated 48%)\n  adding: results_t5base/checkpoint-445/ (stored 0%)\n  adding: results_t5base/checkpoint-445/trainer_state.json (deflated 80%)\n  adding: results_t5base/checkpoint-445/generation_config.json (deflated 29%)\n  adding: results_t5base/checkpoint-445/config.json (deflated 62%)\n  adding: results_t5base/checkpoint-445/training_args.bin (deflated 51%)\n  adding: results_t5base/checkpoint-445/model.safetensors (deflated 8%)\n  adding: results_t5base/checkpoint-445/optimizer.pt (deflated 8%)\n  adding: results_t5base/checkpoint-445/scheduler.pt (deflated 55%)\n  adding: results_t5base/checkpoint-445/rng_state.pth (deflated 25%)\n  adding: results_t5base/events.out.tfevents.1712724251.1d9c84c08798.34.1 (deflated 67%)\n  adding: results_t5base/events.out.tfevents.1712723658.1d9c84c08798.34.0 (deflated 66%)\n  adding: results_t5base/special_tokens_map.json (deflated 85%)\n  adding: results_t5base/added_tokens.json (deflated 83%)\n  adding: results_t5base/tokenizer_config.json (deflated 94%)\n  adding: results_t5base/checkpoint-890/ (stored 0%)\n  adding: results_t5base/checkpoint-890/trainer_state.json (deflated 81%)\n  adding: results_t5base/checkpoint-890/generation_config.json (deflated 29%)\n  adding: results_t5base/checkpoint-890/config.json (deflated 62%)\n  adding: results_t5base/checkpoint-890/training_args.bin (deflated 51%)\n  adding: results_t5base/checkpoint-890/model.safetensors (deflated 8%)\n  adding: results_t5base/checkpoint-890/optimizer.pt (deflated 8%)\n  adding: results_t5base/checkpoint-890/scheduler.pt (deflated 56%)\n  adding: results_t5base/checkpoint-890/rng_state.pth (deflated 25%)\n","output_type":"stream"}]}]}