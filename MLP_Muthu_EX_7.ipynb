{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3f3359",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d17ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a18226",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd865b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('gigaword/train.jsonl','r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c72801",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea2eebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gigaword-train-0</td>\n",
       "      <td>australia 's current account deficit shrunk by...</td>\n",
       "      <td>australian current account deficit narrows sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gigaword-train-1</td>\n",
       "      <td>at least two people were killed in a suspected...</td>\n",
       "      <td>at least two dead in southern philippines blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gigaword-train-2</td>\n",
       "      <td>australian shares closed down #.# percent mond...</td>\n",
       "      <td>australian stocks close down #.# percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gigaword-train-3</td>\n",
       "      <td>south korea 's nuclear envoy kim sook urged no...</td>\n",
       "      <td>envoy urges north korea to restart nuclear dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gigaword-train-4</td>\n",
       "      <td>south korea on monday announced sweeping tax r...</td>\n",
       "      <td>skorea announces tax cuts to stimulate economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gigaword-train-5</td>\n",
       "      <td>taiwan share prices closed down #.## percent m...</td>\n",
       "      <td>taiwan shares close down #.## percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gigaword-train-6</td>\n",
       "      <td>australian shares closed down #.# percent mond...</td>\n",
       "      <td>australian stocks close down #.# percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gigaword-train-7</td>\n",
       "      <td>spanish property group colonial , struggling u...</td>\n",
       "      <td>spain 's colonial posts #.## billion euro loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gigaword-train-8</td>\n",
       "      <td>libyan leader moamer kadhafi monday promised w...</td>\n",
       "      <td>kadhafi promises wide political economic reforms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gigaword-train-9</td>\n",
       "      <td>the united nations ' humanitarian chief john h...</td>\n",
       "      <td>un 's top aid official arrives in drought-hit ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                               text  \\\n",
       "0  gigaword-train-0  australia 's current account deficit shrunk by...   \n",
       "1  gigaword-train-1  at least two people were killed in a suspected...   \n",
       "2  gigaword-train-2  australian shares closed down #.# percent mond...   \n",
       "3  gigaword-train-3  south korea 's nuclear envoy kim sook urged no...   \n",
       "4  gigaword-train-4  south korea on monday announced sweeping tax r...   \n",
       "5  gigaword-train-5  taiwan share prices closed down #.## percent m...   \n",
       "6  gigaword-train-6  australian shares closed down #.# percent mond...   \n",
       "7  gigaword-train-7  spanish property group colonial , struggling u...   \n",
       "8  gigaword-train-8  libyan leader moamer kadhafi monday promised w...   \n",
       "9  gigaword-train-9  the united nations ' humanitarian chief john h...   \n",
       "\n",
       "                                             summary  \n",
       "0  australian current account deficit narrows sha...  \n",
       "1    at least two dead in southern philippines blast  \n",
       "2           australian stocks close down #.# percent  \n",
       "3  envoy urges north korea to restart nuclear dis...  \n",
       "4     skorea announces tax cuts to stimulate economy  \n",
       "5              taiwan shares close down #.## percent  \n",
       "6           australian stocks close down #.# percent  \n",
       "7     spain 's colonial posts #.## billion euro loss  \n",
       "8   kadhafi promises wide political economic reforms  \n",
       "9  un 's top aid official arrives in drought-hit ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2a341",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3d9b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lowercasing\n",
    "data = data.applymap(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a450d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing `#`\n",
    "data = data.applymap(lambda x: str(x).replace('#', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37524fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding `start` nd `end` tokens to the summaries\n",
    "data['summary'] = data['summary'].apply(\n",
    "\n",
    "    lambda x: 'START_ ' + x + ' _END'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d833b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    START_ australian current account deficit narr...\n",
       "1    START_ at least two dead in southern philippin...\n",
       "Name: summary, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['summary'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a7b11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05292c58",
   "metadata": {},
   "source": [
    "### Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf0d5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_text = set()\n",
    "for t in data['text']:\n",
    "    for word in t.split():\n",
    "        if word not in vocab_text:\n",
    "            vocab_text.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14e0bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15845"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0e635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_summary = set()\n",
    "for t in data['summary']:\n",
    "    for word in t.split():\n",
    "        if word not in vocab_summary:\n",
    "            vocab_summary.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a925e076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8757"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f86180",
   "metadata": {},
   "source": [
    "### Data Params Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad3e0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = sorted(list(vocab_text))\n",
    "target_words = sorted(list(vocab_summary))\n",
    "num_encoder_tokens = len(vocab_text)\n",
    "num_decoder_tokens = len(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27056fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15845, 8757)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_tokens,num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34d6face",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length_text_sentence']=data['text'].apply(lambda x:len(x.split(\" \")))\n",
    "data['length_sum_sentence']=data['summary'].apply(lambda x:len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d171d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_src = data['length_text_sentence'].max()\n",
    "max_length_tar = data['length_sum_sentence'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb34bc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 25)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_src,max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "891570ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##For zero-pad\n",
    "num_decoder_tokens+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8be95",
   "metadata": {},
   "source": [
    "### Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "699ce4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80e720bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6f0a2",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96d6bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data['text'], data['summary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39a7e440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000,), (2000,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "477b7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save the data for future purpose\n",
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31af84e",
   "metadata": {},
   "source": [
    "### Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61e13310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] ##encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] ##decoder input seq\n",
    "                    if t>0:\n",
    "                        ##decoder target sequence (one hot encoded)\n",
    "                        ##does not include the START_ token\n",
    "                        ##Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "443b3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0, len(X_train), 128):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X_train[j:j+batch_size], y_train[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] ##encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] ##decoder input seq\n",
    "                    if t>0:\n",
    "                        ##decoder target sequence (one hot encoded)\n",
    "                        ##does not include the START_ token\n",
    "                        ##Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655a3ad",
   "metadata": {},
   "source": [
    "### Encoder Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "740f1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9850182",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf377389",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, hidden_state, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(hidden_state, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df8034",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f313ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, hidden_state, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(hidden_state, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94900b7",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29a36fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39d873a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ff1b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, None, 300)            4753500   ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, None, 300)            2627400   ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               [(None, 300),                721200    ['embedding_2[0][0]']         \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               [(None, None, 300),          721200    ['embedding_3[0][0]',         \n",
      "                              (None, 300),                           'lstm_2[0][1]',              \n",
      "                              (None, 300)]                           'lstm_2[0][2]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 8758)           2636158   ['lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11459458 (43.71 MB)\n",
      "Trainable params: 11459458 (43.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798d963",
   "metadata": {},
   "source": [
    "### Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c2d3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 64\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb50c4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3027cc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/125 [=======================>......] - ETA: 8s - loss: 2.6591WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 12500 batches). You may need to use the repeat() function when building your dataset.\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 2.6591 - val_loss: 11.0432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2726ff10690>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          validation_split=0.2,\n",
    "         steps_per_epoch = train_samples//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0190b6",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2713d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('n_sq_comp_t_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c7279561",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f889b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) ##Get the embeddings of the decoder sequence\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) ##A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "##Decoder\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "60eb935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    ##Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    ##Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        ##Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        ##Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce58dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "29eb0308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Input Text sentence: china 's only wholly chinese-owned passenger car production line has launched an expansion project to more than triple production in  , the overseas edition of the people 's daily said saturday .\n",
      "Actual Summary:  tianjin automobile launches massive expansion drive \n",
      "Predicted Summary:  us condemns of markets in on bid \n"
     ]
    }
   ],
   "source": [
    "k+=6\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input Text sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Summary:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Summary:', decoded_sentence[:-4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
