{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5c2fd8",
   "metadata": {},
   "source": [
    "# Muthu Palaniappan M - 21011101079 NLP LAB EX -2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a6242",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bbb17c",
   "metadata": {},
   "source": [
    "1. **Data Gathering**\n",
    "   - Source: Kaggle - https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset\n",
    "   - Labels: Netural, Negative, Positive\n",
    "\n",
    "2. **Data Preprocessing**\n",
    "      - Removing irrelevant characters, symbols, and numbers.\n",
    "      - Tokenization\n",
    "      - Removing stop words\n",
    "      - Lemmatization and stemming to reduce words to their base form. (I did both in this notebook for my Lab ex)\n",
    "\n",
    "3. **Feature Extraction**\n",
    "   - Processed text data into numerical features -> input for machine learning model.\n",
    "   - Bag of Words.\n",
    "   - TF-IDF.\n",
    "   - Skipgram.\n",
    "   - CBOW.\n",
    "\n",
    "4. **Model Choice**\n",
    "      - Naive Bayes classifier.\n",
    "      - Decision Tree with depth 5\n",
    "\n",
    "5. **Model Training**\n",
    "   - Dataset into training and validation sets.\n",
    "   - Train it\n",
    "\n",
    "6. **Model Evaluation**\n",
    "   - Metrics -> accuracy, precision, recall, and F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71810bf1",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154f2a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "import string as st\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc0a01",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9041012c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
       "1                             Sooo SAD  negative          noon       21-30   \n",
       "2                          bullying me  negative         night       31-45   \n",
       "3                       leave me alone  negative       morning       46-60   \n",
       "4                        Sons of ****,  negative          noon       60-70   \n",
       "\n",
       "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
       "0  Afghanistan          38928346         652860.0               60  \n",
       "1      Albania           2877797          27400.0              105  \n",
       "2      Algeria          43851044        2381740.0               18  \n",
       "3      Andorra             77265            470.0              164  \n",
       "4       Angola          32866272        1246700.0               26  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\",encoding='unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0ae373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         selected_text sentiment\n",
       "0  I`d have responded, if I were going   neutral\n",
       "1                             Sooo SAD  negative\n",
       "2                          bullying me  negative\n",
       "3                       leave me alone  negative\n",
       "4                        Sons of ****,  negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['selected_text','sentiment']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ddc323d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text sentiment\n",
       "0  I`d have responded, if I were going   neutral\n",
       "1                             Sooo SAD  negative\n",
       "2                          bullying me  negative\n",
       "3                       leave me alone  negative\n",
       "4                        Sons of ****,  negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns={\"selected_text\":\"text\"},inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085beab2",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ab0cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       27480 non-null  object\n",
      " 1   sentiment  27481 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 429.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d995a7",
   "metadata": {},
   "source": [
    "#### 1. Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c42f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    removed_text = \"\"\n",
    "    for char in str(text):\n",
    "        if char not in st.punctuation:\n",
    "            removed_text+=char\n",
    "    return removed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d52ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['removed_punc'] = data['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "612f6dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: I`d have responded, if I were going\n",
      "After: Id have responded if I were going\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {data['text'][0]}\\nAfter: {data['removed_punc'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe811d",
   "metadata": {},
   "source": [
    "#### 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd1643c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens(text):\n",
    "    text = str(text).lower()\n",
    "    tokens = []\n",
    "    tokens = re.split(\"\\s+\",text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e496505",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tokens'] = data['removed_punc'].apply(convert_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da9e2ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['id', 'have', 'responded', 'if', 'i', 'were', 'going']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokens: {data['Tokens'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381aeed",
   "metadata": {},
   "source": [
    "#### 3.Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ba977aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "  return [token for token in tokens if token not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b7f21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['removed_stopwords_tokens'] = data['Tokens'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "320c464f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['id', 'have', 'responded', 'if', 'i', 'were', 'going']\n",
      "After: ['id', 'responded', 'going']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {data['Tokens'][0]}\\nAfter: {data['removed_stopwords_tokens'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85150f",
   "metadata": {},
   "source": [
    "#### 4.Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa7bc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(tok) for tok in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb26ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stemming_tokens'] = data['removed_stopwords_tokens'].apply(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a3311cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['id', 'responded', 'going']\n",
      "After: ['id', 'respond', 'go']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {data['removed_stopwords_tokens'][0]}\\nAfter: {data['stemming_tokens'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f076e",
   "metadata": {},
   "source": [
    "#### 5.Lemma Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3925125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lema_tokens(tokens):\n",
    "    word_net = WordNetLemmatizer()\n",
    "    tokens = [word_net.lemmatize(tok) for tok in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6acbed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemma_tokens'] = data['removed_stopwords_tokens'].apply(lema_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f203e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['id', 'responded', 'going']\n",
      "After: ['id', 'responded', 'going']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {data['removed_stopwords_tokens'][0]}\\nAfter: {data['lemma_tokens'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f259c",
   "metadata": {},
   "source": [
    "#### 6. Return Pre-Processed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d92e256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sequence(tokens):\n",
    "  return \" \".join([token for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "196a43ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pre_processed_text'] = data['lemma_tokens'].apply(return_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91793e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['id', 'responded', 'going']\n",
      "After: id responded going\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {data['lemma_tokens'][0]}\\nAfter: {data['pre_processed_text'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09e052cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ea4aa",
   "metadata": {},
   "source": [
    "### Feature Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a004d200",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25b96855",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(data['pre_processed_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5592545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27480, 17424)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616d4e10",
   "metadata": {},
   "source": [
    "##### Impact on BoW\n",
    "- BoW may struggle with capturing semantic meaning and context, leading to misclassification. \n",
    "- It treats each word independently, ignoring word relationships.\n",
    "- OOV Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05e1fb",
   "metadata": {},
   "source": [
    "#### Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5c3c125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id responded going',\n",
       " 'sooo sad',\n",
       " 'bullying',\n",
       " 'leave alone',\n",
       " 'son ',\n",
       " 'httpwwwdothebouncycomsmf shameless plugging best ranger forum earth',\n",
       " 'fun',\n",
       " 'soooo high',\n",
       " '',\n",
       " 'wow u became cooler',\n",
       " 'much love hopeful reckon chance minimal p im never gonna get cake stuff',\n",
       " 'like',\n",
       " 'dangerously',\n",
       " 'lost',\n",
       " 'test test lg env2',\n",
       " 'uh oh sunburned',\n",
       " 'sigh',\n",
       " 'sick',\n",
       " 'onna',\n",
       " 'he',\n",
       " 'oh marly im sorry hope find soon 3 3',\n",
       " 'interesting',\n",
       " 'cleaning house family comming later today',\n",
       " 'gotta restart computer thought win7 supposed put end constant rebootiness',\n",
       " 'see wat mean bout foll0w friidays called lose f0llowers friday smh',\n",
       " 'free fillin app ipod fun im addicted',\n",
       " 'im sorry',\n",
       " 'internet',\n",
       " 'fun',\n",
       " 'power back working',\n",
       " 'quiteheavenly',\n",
       " 'hope',\n",
       " 'well much unhappy 10 minute',\n",
       " 'funny',\n",
       " 'ahhh slept game im gonna try best watch tomorrow though hope play army',\n",
       " 'thats end tear fear',\n",
       " 'miss',\n",
       " 'case wonder really busy today coming adding ton new blog update stay tuned',\n",
       " 'soooooo sleeeeepy',\n",
       " 'little happy fo',\n",
       " 'car happy big big dent boot hoping theyre going write crossing finger waiting',\n",
       " 'avid fan',\n",
       " 'mayday',\n",
       " 'ratt rocked nashville toniteone thing sucked encore like 80 still fun show pearcy hott bad boy look',\n",
       " 'love',\n",
       " 'girl hair salon asked shall trim eyebrow old feel',\n",
       " 'suckkkkkk',\n",
       " 'visiting friendster facebook',\n",
       " 'dont like go',\n",
       " 'im thrilled mine',\n",
       " 'check httptwittersuckscom connect tweeple hate twitter',\n",
       " 'also bored school third freelesson freistunde ',\n",
       " 'hm u guess',\n",
       " 'u dissappointed past day',\n",
       " 'romance zero funny',\n",
       " 'id rather early runbut morning runner',\n",
       " 'hurt',\n",
       " 'back later',\n",
       " 'torn ace heart',\n",
       " 'fun speaking',\n",
       " 'lost friend im alone sleepy',\n",
       " 'haha yes',\n",
       " 'give easily',\n",
       " 'favorite',\n",
       " 'jealous',\n",
       " 'photoshoot',\n",
       " 'awesome',\n",
       " 'yay playing show tonight boo gonna soggy im work right playing',\n",
       " 'chilliin',\n",
       " 'know agent let know',\n",
       " 'still smell smoke kitchenfire',\n",
       " 'better',\n",
       " 'anyone extra keane ticket promise buy drink take rad pic fb blog flickr etc',\n",
       " 'ride one catch one summer til pop open one ',\n",
       " 'good gorjuz yea kno asked yesterday tha hospital talked u said',\n",
       " 'ok im popped say hi check thing ill probably head guttah later tonight',\n",
       " 'baddd',\n",
       " 'source say',\n",
       " 'sooo tired',\n",
       " 'hey change twitter account didnt even tell',\n",
       " 'thank yyyyyyyyyoooooooooouuuuu',\n",
       " 'lucky',\n",
       " 'fell asleep waiting ride',\n",
       " 'sick',\n",
       " ' sorry guy',\n",
       " 'happy star war day everyone enjoy holiday uk',\n",
       " 'mile im essex give plenty warning arrive time get least one free beer',\n",
       " 'snoring annoying n keep sleeping like right lol honestly wud miss eva left love',\n",
       " 'miss bby',\n",
       " 'cool',\n",
       " 'love',\n",
       " 'mounce yes last way past bedtime',\n",
       " 'hi joined twitter',\n",
       " 'tired',\n",
       " 'eating ice cream getting ready graduation',\n",
       " 'happy mother day mum',\n",
       " 'freaked',\n",
       " 'unfortunately',\n",
       " 'gonna read story bout adam lambert online bed nighty night',\n",
       " 'best',\n",
       " 'pretty',\n",
       " 'certainly cheer huh',\n",
       " 'horrible',\n",
       " 'busy',\n",
       " 'awesome',\n",
       " 'least get watch time let go pen',\n",
       " 'cool wear black time go',\n",
       " 'thanks',\n",
       " 'safe',\n",
       " 'wish allowed go',\n",
       " 'u friendster add email adress add lococrime1stcom add',\n",
       " 'ticket',\n",
       " 'thank',\n",
       " 'acsm unfathomable think one one kept comfort bedroom yes',\n",
       " 'love',\n",
       " 'dont feel confident',\n",
       " 'sad',\n",
       " 'hahaa awesomee ',\n",
       " 'awesomeeeee',\n",
       " 'hate fallout 3 keep making jump im also low health money ammo food dont worry ill get',\n",
       " 'itunes lost song',\n",
       " 'whats gloomy weather sun must tired come play heading victoria garden impulse buy haha',\n",
       " 'looking forward',\n",
       " 'poor',\n",
       " 'well',\n",
       " 'prob',\n",
       " 'dad watching mtv going sims2 minutee',\n",
       " 'absolutely',\n",
       " 'whats matter chickadee',\n",
       " 'adore',\n",
       " 'good',\n",
       " 'love',\n",
       " 'painful',\n",
       " 'sad',\n",
       " 'e nice',\n",
       " 'wish',\n",
       " 'namaskar namaste r marathi people say namaskar marathi word naaaah ',\n",
       " 'congrats cuss like matter minute didnt know reward',\n",
       " 'humous doritos oh yes',\n",
       " 'missed awesome weather',\n",
       " 'today going normal day hope group pilot large airline come last night much drink',\n",
       " 'terrible',\n",
       " 'unfortunatley',\n",
       " 'suck tho',\n",
       " 'hate fighting',\n",
       " 'watched didnt want win put good fightlol',\n",
       " 'carwarmed sprite taste like sore throat',\n",
       " 'came 11th cross country beat dumbo',\n",
       " 'enjoyable',\n",
       " 'endearing',\n",
       " 'tomorrow valerias lunch going get hair done im arraving late got cousin babtizm whatever spell',\n",
       " 'goooooddd morning tweet',\n",
       " 'hate',\n",
       " 'fine',\n",
       " 'dont like one',\n",
       " 'mmmmmmmm morning',\n",
       " 'neither',\n",
       " '10 hour work sunday boo find time two hour lunchbreak though yeah',\n",
       " 'bugger forgot still washing machine',\n",
       " 'sending love blessing healing thought family peace',\n",
       " 'really bad',\n",
       " 'ah yes know feeling',\n",
       " 'night cooker dad',\n",
       " 'brutal',\n",
       " 'nope coquitlam',\n",
       " 'boring',\n",
       " 'p sound like fun',\n",
       " 'big booming thunder storm almost maybe go home early ah probably',\n",
       " 'great',\n",
       " 'excited',\n",
       " 'good morning',\n",
       " 'best show ever',\n",
       " 'messed',\n",
       " 'think iv hurt tooth eilish cassie drawing competiton draw cooky pineapple haha l ',\n",
       " 'want know audition mander text orreply please',\n",
       " 'secret namerebecca please',\n",
       " 'miss',\n",
       " 'need get computer fixed',\n",
       " 'illness',\n",
       " 'cool people want find following today english guess english dont tweet',\n",
       " 'siri woulda put honeybut dont',\n",
       " 'totally loved',\n",
       " '3 u helped thru hrdest time life',\n",
       " 'sad',\n",
       " 'finally got call marriage counseling 3 day late',\n",
       " 'ok',\n",
       " 'baby',\n",
       " 'sick',\n",
       " 'impromptu pool party except dont know swim cant get',\n",
       " 'oww',\n",
       " 'happy 1 year 3',\n",
       " 'goooooooooooood morrrrrrrrning',\n",
       " 'phew make note case anyone else run issueã¯â¿â½',\n",
       " 'vote every day ',\n",
       " 'diet killing',\n",
       " 'talk',\n",
       " 'bored',\n",
       " 'e fun',\n",
       " 'far good sleep 4 hour getting bit twitchy',\n",
       " 'whats twatter lately either cant get reply dont turn',\n",
       " 'lost voice',\n",
       " 'hate',\n",
       " 'ive heard fall im waiting',\n",
       " 'nightmare huggles',\n",
       " 'creeper feel disappointed cyberstalking skill internet privacy',\n",
       " 'headache',\n",
       " 'happy',\n",
       " 'grabbing coffee making mom breakfast',\n",
       " 'going fun',\n",
       " 'thanks major chop',\n",
       " 'thank',\n",
       " 'got updated ipod',\n",
       " 'hahaha',\n",
       " 'crush',\n",
       " 'saw james carville store today head really bald',\n",
       " 'yellow httpblipfm5z05g',\n",
       " 'mean youre going come back vancouver way hahah',\n",
       " 'feeling smooth',\n",
       " 'ew traffic',\n",
       " 'downloading song trying sneak lil homework main priority song lol',\n",
       " 'alonei need coffee',\n",
       " 'sound like',\n",
       " 'bad day day realize mess youve put one happiest day life',\n",
       " 'hate bike',\n",
       " 'nesmith',\n",
       " 'worse tax',\n",
       " 'jonas brother live party rocking hard',\n",
       " 'happy',\n",
       " 'would love test though',\n",
       " 'fun night',\n",
       " 'exception short dude larenz fineass tate yum',\n",
       " 'thought wolverine awesome',\n",
       " 'hopefully',\n",
       " 'yes work 6 3',\n",
       " 'hmmm maybe thats meant eluded something brand new know medium',\n",
       " 'done spa meeting vic late lunch',\n",
       " 'happy',\n",
       " 'always wanted go oz',\n",
       " 'thx',\n",
       " 'luv',\n",
       " 'thats need',\n",
       " 'okay im dedicating 300th tweet fact im going apple store huge crack glass screen',\n",
       " 'could ever actually allowed stay',\n",
       " 'let know turn',\n",
       " 'sas detect long isnt back sas haha',\n",
       " 'ahaha',\n",
       " 'sport bar shatranjanpoli rest ph 26498457 sport bar andheri w 26733333 dont know whether help google ki jai ho',\n",
       " 'im sleeping un',\n",
       " 'seriously',\n",
       " 'live pain bring',\n",
       " 'okay im back later',\n",
       " 'g powerblog challenge keep talking im newbie followe',\n",
       " 'died',\n",
       " 'waiting tish get got drive mom crv pick duckie first time',\n",
       " 'going well',\n",
       " 'going shower dont want smell school tomorrow',\n",
       " 'sigh know',\n",
       " 'hopefully',\n",
       " '4 free twitter tool get follower httpshortto511q httpjijrcomhulz httpshortto511r http2veorgxpg0',\n",
       " 'got home work chugging big bottle apple juice',\n",
       " 'g harmed',\n",
       " 'amazing day',\n",
       " 'big brotherand im done',\n",
       " 'good news finally finished easactive workout paused 6 hour bad news resistance band torn',\n",
       " 'well good morning wonderful day neighborhood thanks following another 60 morning',\n",
       " 'amazing tonight',\n",
       " 'hope',\n",
       " 'hurt much',\n",
       " 'waited listening wind blowing tumbleweed none old enough know someone say crackerack',\n",
       " 'hell yeah',\n",
       " 'shirt dinner need ask actually ville',\n",
       " 'know',\n",
       " 'huh smelly noooo love alex vixon',\n",
       " 'great',\n",
       " 'happy bday',\n",
       " 'thank',\n",
       " 'happy',\n",
       " 'glad',\n",
       " 'thanks',\n",
       " 'twittering 2 day',\n",
       " 'nice',\n",
       " 'depressed',\n",
       " 'sick',\n",
       " 'happy mother day people love mom lot still',\n",
       " 'sorry',\n",
       " ' soooo friendly',\n",
       " 'happy',\n",
       " 'getting hang twitter',\n",
       " 'know neck jacked forced pay parking bc cant turn head parallel park free space',\n",
       " 'want back',\n",
       " 'miss',\n",
       " 'bathing two little angel keyla janice',\n",
       " 'chocked',\n",
       " 'hahahha lol true always remember bd never remember date even day',\n",
       " 'family herehanging',\n",
       " 'gorgeous',\n",
       " 'worried',\n",
       " 'im bannished',\n",
       " 'morning get see ill let know right im going go see wolverine',\n",
       " 'red top tabloid build em knock em',\n",
       " 'tonight party w girl minus vita',\n",
       " 'made sad thought youd jumping joy',\n",
       " 'simple',\n",
       " 'pic uploaded baby pic cat missy adult pretty little kitty batty kitten heaven',\n",
       " 'amazing',\n",
       " 'glad',\n",
       " 'missed',\n",
       " 'live thats want better bound bad egg though soon learn',\n",
       " 'hell yeah kellynn got twitter finally',\n",
       " 'wort',\n",
       " 'better',\n",
       " 'ship im stuck',\n",
       " 'cannot wait',\n",
       " 'shame',\n",
       " 'look like office tv get mlb network look like mlbn televising detbal game today wieters',\n",
       " 'home empty handed comic found today shall indulge cupcake magnolia bakery',\n",
       " 'definitely pinched nerve',\n",
       " 'exhausting',\n",
       " 'going go sunday ive got much going weekend',\n",
       " 'sorry',\n",
       " 'im sad hurt',\n",
       " 'get distracted id like thank new follower taking trouble follow others feelin love',\n",
       " 'im broke',\n",
       " 'sure didnt say 2 big jst saw pic u ur last bday looked pretty miss',\n",
       " 'sore throat planning tet outing marwell thoughgood time',\n",
       " 'amazin',\n",
       " 'migraine',\n",
       " '',\n",
       " 'suck',\n",
       " 'whens sway sway winner announced',\n",
       " 'k check',\n",
       " 'pretty well',\n",
       " 'want wake early get coffee tomorrow today going busyy day keep writing booo whoo',\n",
       " 'miss daddy mommy',\n",
       " 'officially depressed',\n",
       " 'realized cant forward text msg iphone',\n",
       " 'exciting',\n",
       " 'suck',\n",
       " 'bored',\n",
       " 'lost',\n",
       " 'peter gordon morning go piece wanna hide go piece almost die ever httpblipfm5yk38',\n",
       " 'bad',\n",
       " 'lmao smh one threw',\n",
       " 'oh nice going',\n",
       " 'getting ready work working weekend',\n",
       " 'partying',\n",
       " 'agree',\n",
       " 'hopeless everything else',\n",
       " 'perfect',\n",
       " 'thanks',\n",
       " 'sorry',\n",
       " 'saw amazing heeels big',\n",
       " 'stuck',\n",
       " 'dang',\n",
       " 'missed ur tweet',\n",
       " 'damnit suck',\n",
       " 'bos shes moving nyc',\n",
       " 'sooo crazy',\n",
       " 'stupid ',\n",
       " 'think need new friend',\n",
       " 'looking forward gig ireland see ya',\n",
       " 'please',\n",
       " 'aww loooove',\n",
       " 'hungry twitter want food',\n",
       " 'awesome lucky',\n",
       " 'yea know tell everything p send direct message telling ha',\n",
       " 'love',\n",
       " 'great time beer garden wit boyos think sun got bit though feel bit ill ',\n",
       " 'fun',\n",
       " 'u really dont think maybe ur rightlol btw phone u using think u told b4i might app',\n",
       " 'feel pain',\n",
       " 'come home two day',\n",
       " 'sadly',\n",
       " 'cant wait',\n",
       " 'ehhh check dentist app next week though getting molar pulledroot canal',\n",
       " 'aww miss driving',\n",
       " 'omg wango tango awsome love baby taking',\n",
       " 'cool',\n",
       " 'wish',\n",
       " 'yeah real hard know youll get smile',\n",
       " 'thats good',\n",
       " 'made night way funny',\n",
       " 'happy',\n",
       " 'make face get home mom watching soap',\n",
       " 'go work',\n",
       " 'cant believe went got boba without',\n",
       " 'today',\n",
       " 'thanks',\n",
       " '19 day counting',\n",
       " 'sorry friend',\n",
       " 'brainfreeze',\n",
       " 'discovered shortcoming gravity use twitpic integrated doesnt subtract pic url 140 character limit',\n",
       " 'sunburn peeling',\n",
       " 'perky purple nail polish isnt perky chipped',\n",
       " 'httptwitpiccom4sx96 put camera smoker pit longer vandalize door without caught',\n",
       " 'dont one cuddle',\n",
       " 'hate',\n",
       " 'sorry',\n",
       " 'dying',\n",
       " 'qood morninq',\n",
       " 'writing english original writing storyyyyy listening little respect erasure aaaaaah',\n",
       " 'need follower',\n",
       " 'stupid job',\n",
       " 'little wormy labyrinth sadly passed away today ok he still around happy ghost form',\n",
       " 'pant idea could new attempt worldwide attract business back airline',\n",
       " 'good',\n",
       " 'popular',\n",
       " 'dosent want',\n",
       " 'voice great',\n",
       " 'wish birthday massacre would come australia think said theyre thinking tho',\n",
       " 'hoping go red lobster weekend',\n",
       " 'lucky',\n",
       " 'sure hope',\n",
       " 'congrats',\n",
       " 'passed away',\n",
       " 'amazing people',\n",
       " 'cool',\n",
       " 'rice crusty bread chili aawww',\n",
       " 'like mind evening dear',\n",
       " 'come save packing please',\n",
       " 'yes love tea',\n",
       " 'mouth sure',\n",
       " 'finally 1 hour history 300 pm went highschool 800 make homework friend',\n",
       " 'thanks',\n",
       " 'dont entertain ',\n",
       " 'kill',\n",
       " 'electronic key stopped working keyhole',\n",
       " 'good',\n",
       " 'poor tony',\n",
       " 'goodmorning',\n",
       " 'dont really feel like',\n",
       " 'love mine happy motherã¯â¿â½s day mom john taylor much love',\n",
       " 'saw black snake garden went back picture gone',\n",
       " 'best',\n",
       " 'waterfront anymore',\n",
       " 'love mum much happy mother day wonderful mother',\n",
       " 'feeling like poop',\n",
       " 'excited',\n",
       " 'cry ',\n",
       " 'discovered sharing g reader didnt even know sigh im g reader newb',\n",
       " 'chilled tonite cannot spew venom write funny seems like 2 style',\n",
       " 'u please pray 4 lord know need',\n",
       " 'aaaaaw want live usa',\n",
       " 'morning tweeple',\n",
       " 'hey didnt get ',\n",
       " 'name',\n",
       " 'make day much better rough one mention love new photo',\n",
       " 'dont want sit home',\n",
       " 'wishing',\n",
       " 'concert tonight chackin coming tomorrow',\n",
       " 'happened thought coming back today',\n",
       " 'dammit',\n",
       " 'lol',\n",
       " 'still jealous',\n",
       " 'forever 21 ethan couldnt',\n",
       " 'sad',\n",
       " 'easy',\n",
       " 'wow beautiful pictur',\n",
       " 'look good',\n",
       " 'people pencil sharp sharpened pencil philosophy',\n",
       " 'go home im usually mr positive one daze well 15 min tomorrow',\n",
       " 'always lee let go paris',\n",
       " 'nice',\n",
       " 'lot fun',\n",
       " 'bouncing rush make feel nauseous',\n",
       " 'lovely day',\n",
       " 'playing singstar without fave duetter',\n",
       " 'like',\n",
       " 'thats weird ',\n",
       " 'im good daughter',\n",
       " 'thats cool',\n",
       " 'alwas help',\n",
       " 'lacking',\n",
       " 'cool',\n",
       " 'good',\n",
       " 'think honeymoon good life either honey moon joseph arthur',\n",
       " 'swollen',\n",
       " 'hour ago came realised went buy something online',\n",
       " 'oh referring lil exchange lj regard twitter archive posting',\n",
       " 'dont act like u dont know',\n",
       " 'best',\n",
       " ' im scared',\n",
       " 'missing',\n",
       " 'mean',\n",
       " 'good',\n",
       " 'back soon need run shop cut grass',\n",
       " 'im plan transform bedroom today random',\n",
       " 'missing',\n",
       " 'thats another sponsor',\n",
       " 'stuck',\n",
       " 'sad',\n",
       " 'sorr',\n",
       " 'home alone hw',\n",
       " 'live poverty',\n",
       " 'going well',\n",
       " 'uploading bamboozle picture facebook',\n",
       " 'congratulation',\n",
       " 'miss',\n",
       " 'woke upi wanna stay bed',\n",
       " 'wack',\n",
       " 'would great vip act id enjoy seeing noooo',\n",
       " 'wish could make hate commute sometimes',\n",
       " 'hopefully make good tip',\n",
       " 'wasnt chance',\n",
       " 'hate',\n",
       " 'thanks support',\n",
       " 'there really android twitter app tweeties calibre',\n",
       " 'lovely',\n",
       " 'nyappy mother day mom',\n",
       " 'yeah im gonna take ur picture ipod baby',\n",
       " 'traumatizing',\n",
       " 'lol herewish way microsize everythinglol',\n",
       " 'fail',\n",
       " 'glad',\n",
       " 'dinner evening playing card already packed ready head home tomorrow go home',\n",
       " 'im allergic cat tonsil get swollen hurt dooo',\n",
       " ' tomorrow tough',\n",
       " 'miss',\n",
       " 'im glad',\n",
       " 'thanks',\n",
       " 'hope',\n",
       " 'well',\n",
       " 'nah jkin he hot bored',\n",
       " 'ahhh im sqeaky clean fresh even though im wearing dirty clothes love two half men amazing',\n",
       " 'yes read many time',\n",
       " 'cuz play grown twitter',\n",
       " 'living ignorance',\n",
       " 'nope telling want cameo cream',\n",
       " 'awwww gonna miss',\n",
       " 'relaxing place',\n",
       " 'couldnt cost part',\n",
       " 'lovely',\n",
       " ' sorry',\n",
       " 'dammit',\n",
       " 'amazing',\n",
       " ' barb trying figure dsl aint connecting need google going home computer dont start working soon smh',\n",
       " 'sick sad ',\n",
       " 'seems really quiet tonightam jealous clearly exciting life bed think',\n",
       " 'really wish spare cash buy new punch wii',\n",
       " 'going',\n",
       " 'sorry',\n",
       " 'feel hope smooth flight safe',\n",
       " 'cant sleep',\n",
       " 'sad',\n",
       " 'sweet dream',\n",
       " 'hay wats ur aim chat',\n",
       " ' okay kool might touring summer long make happen',\n",
       " 'pain',\n",
       " 'time watch op dead like sleep',\n",
       " 'dinner grandma since couldnt mum',\n",
       " 'haha thanks history invention television influence america lol',\n",
       " 'yea wassup',\n",
       " 'stupid',\n",
       " 'shopped til droppedcome bac sunshine miss u',\n",
       " 'like',\n",
       " 'thank ive anxiety issue',\n",
       " 'around 3 hour left 3 day weekend waaaay much work',\n",
       " 'loved',\n",
       " 'attempt somehow extend inner class would close closure cant find ref atm',\n",
       " 'yes join u require cupcake donation',\n",
       " 'hard',\n",
       " 'terrible headache super swollen puffy eye dont think im going todayugh',\n",
       " 'cant access site',\n",
       " 'sun shinningat last',\n",
       " 'slowest train ever',\n",
       " 'nup cd either whole bunch zero one free',\n",
       " 'graduation day feel like failure',\n",
       " 'ops lol wasnt supposed got twitter',\n",
       " 'dont think ive ever tierd lifeu',\n",
       " 'u going today girl',\n",
       " 'doesnt want anymore',\n",
       " 'bad hair day',\n",
       " 'headache',\n",
       " 'sorry',\n",
       " 'okayokay like rest sane population world hate monday',\n",
       " 'mozart requiem',\n",
       " 'think bit love creation wit physically attracted although great style',\n",
       " 'thanks another great day',\n",
       " 'huh',\n",
       " 'ohshnapsss pissed blair usual hahah yeeeah bake cooky',\n",
       " 'bueno hollykins need feel better asap p miss done uni soon arent soproudofyo',\n",
       " 'awesome',\n",
       " 'happy',\n",
       " 'expression watching httptinyurlcomeqbwe',\n",
       " 'stupid',\n",
       " 'didnt want tell think body odour',\n",
       " 'nice beta easports still news online madden',\n",
       " 'bad',\n",
       " 'missed',\n",
       " 'unfortunately',\n",
       " 'fun',\n",
       " 'discrimination bad thing',\n",
       " 'sure talkthe fabulous part though sorry dear',\n",
       " 'tease',\n",
       " 'got back working im feeling pretty good',\n",
       " 'ahhh satz blend didnt save',\n",
       " 'matter bet would great love right go',\n",
       " 'thank heaven',\n",
       " 'ear popping',\n",
       " 'bad',\n",
       " 'answer back',\n",
       " 'catching 2 week lost grey house quiet',\n",
       " 'smile',\n",
       " 'im kinda sick n tired',\n",
       " 'boo',\n",
       " 'hey phillll wazzuppppp',\n",
       " 'missed',\n",
       " 'great',\n",
       " 'finally gone beach yeaaaah',\n",
       " 'awesome',\n",
       " 'last weekday nothing school start next week',\n",
       " 'gorgeous girl',\n",
       " 'hysterical',\n",
       " 'ok turn twitter device update get new battery phone go fully charged dead 5 hour',\n",
       " 'michelle hot mama chichi grande',\n",
       " 'rock much',\n",
       " 'one know like boiled peanut',\n",
       " 'loved',\n",
       " 'ugh tonsil',\n",
       " 'please get cell phone better camera picture real bad quality compared miley took',\n",
       " 'bless',\n",
       " 'good fun',\n",
       " 'lost',\n",
       " 'though isnt going well',\n",
       " 'hoooray im hooked already',\n",
       " 'good',\n",
       " 'agree everybody wouldve excited go ohhhh nooooo m screw',\n",
       " 'edge left contributor list month issue',\n",
       " 'ooo growl yummy time coming soon huh oh yeah hunting combo tshirt think find one',\n",
       " 'im back zindex problemagain',\n",
       " 'bored work',\n",
       " 'hating',\n",
       " 'wwwmyspacecomfashionisthenextcity check started new tee limited edition different way check facebook group',\n",
       " 'husband girl',\n",
       " 'thinking vega good idea place see',\n",
       " 'suck',\n",
       " 'hi one kiwi artist another kiwi artis',\n",
       " 'happy mother day',\n",
       " 'oooo ok havent accepted friend reque',\n",
       " 'sorry',\n",
       " '',\n",
       " 'love good cause',\n",
       " 'available area',\n",
       " 'friday 5 hour im freeuntil tomorrow oh well',\n",
       " 'meeting time iã¯â¿â½m trying win something prize friday',\n",
       " 'cramp',\n",
       " 'thank',\n",
       " 'plan didnt go followed ok',\n",
       " 'let guess skipped sport bought new pinkish outfit',\n",
       " 'rain please come',\n",
       " 'pleasure',\n",
       " 'fml',\n",
       " 'funny',\n",
       " 'grouchy',\n",
       " 'worse',\n",
       " 'dont app',\n",
       " 'would help study aptitude test',\n",
       " 'supper tonight kurumi 10 minute',\n",
       " 'cute',\n",
       " 'loved',\n",
       " 'hi mariah',\n",
       " 'yours13 im home cat right',\n",
       " 'inbox still empty',\n",
       " 'im rebel',\n",
       " 'im sorry',\n",
       " 'nice',\n",
       " 'exam 1 today going get license renewed birthday getting closer',\n",
       " 'love',\n",
       " 'think gonna rain',\n",
       " 'good',\n",
       " 'yrbook signing w evryone fun im gna miss evrything',\n",
       " 'lost',\n",
       " 'close enough could run get one',\n",
       " 'stone cold crazy',\n",
       " 'totally austin power withdrawl symptons darrius withdrawl symptom mommy minne withdrawl syamptoms',\n",
       " 'pretty',\n",
       " 'senior last day',\n",
       " 'one help',\n",
       " 'thanks',\n",
       " 'thanks',\n",
       " ' getting closer rather real lobby date chicago blogher09 squee',\n",
       " 'lost vocie adele laughing lol still wondering im college week',\n",
       " 'oh sorry getting reply nothing',\n",
       " 'pray uncle',\n",
       " ' gonna scare rachel quarantine tonight shall fun',\n",
       " 'omg wanted slap',\n",
       " 'watching simpson',\n",
       " 'special',\n",
       " 'great',\n",
       " 'work soon way soon',\n",
       " 'phaket anneliese want',\n",
       " 'mad',\n",
       " 'thanks',\n",
       " 'im hungry wife bodyshop party bringing takeaway home much longer party go',\n",
       " 'pretty baby',\n",
       " 'invite house instead',\n",
       " 'im bit ben thaied',\n",
       " 'stackeoverflow',\n",
       " 'cute',\n",
       " 'dead stopped express lane would happen choose take way ugh hopefully get moving',\n",
       " 'okeefe yay youre twitter youre secret celeb crush much info',\n",
       " 'k thank',\n",
       " 'fail life sometimes',\n",
       " 'wish',\n",
       " 'point gear question posted cant get rest dreadweave set',\n",
       " 'remind much omaha girl use date guess jean calvin klein wore',\n",
       " ' give twitter tip hope help',\n",
       " 'loved',\n",
       " ' lol',\n",
       " 'gorgeous',\n",
       " 'guess relaxing dinner movie tonighti looking forward day work',\n",
       " 'laptop grew speaker watch charlie sound',\n",
       " 'rofl uh huh',\n",
       " 'funny',\n",
       " 'doesnt look fab',\n",
       " 'deadline',\n",
       " 'terrible',\n",
       " 'irrelevant',\n",
       " 'im sick go outside',\n",
       " 'good',\n",
       " 'excited',\n",
       " 'wont home till like next month',\n",
       " 'love',\n",
       " 'bad',\n",
       " 'screwed',\n",
       " 'got laid lot people becoming unemployed',\n",
       " 'e bestttt',\n",
       " 'easties go anytime',\n",
       " 'like',\n",
       " 'lemme guess ran 5 mile gym waking kid going beach recording studio',\n",
       " 'never knew dentention hard',\n",
       " 'fun',\n",
       " 'nail broke haaaaaaaaate',\n",
       " 'disappointing',\n",
       " 'yep',\n",
       " 'love',\n",
       " ' hope avalina isnt dud',\n",
       " 'anyone want buy anthropomorphic planter httptinyurlcomm6sru3 available till 31st',\n",
       " 'beautiful day outside today shame im stuck office blind shut stop glare',\n",
       " 'woke sun started rain',\n",
       " 'creative vados stock walmartcom missed thanks tweeting',\n",
       " 'hate',\n",
       " 'lovely',\n",
       " 'flapataco nice pleb came',\n",
       " 'celebrate',\n",
       " 'chillaxin busy bankholiday hope everbody gd wkend holiday 12 day',\n",
       " 'better',\n",
       " 'wowyou ride hard',\n",
       " 'check flyer designed notary retrograde httprachellovespeaceblogspotcom let know think',\n",
       " 'thanks welcome back',\n",
       " 'suck',\n",
       " 'sore head',\n",
       " 'office trying solve mystery whose blood bathroom toilet eewwwww',\n",
       " 'awwwwi love car soooo cute tonight love carlmy 10yr old sad nowlol',\n",
       " 'bryan hasnt replied wingnuts',\n",
       " 'watching southpark another 20 minute',\n",
       " 'e amazi',\n",
       " 'lmaoha',\n",
       " 'stupid',\n",
       " 'tried shoe new look',\n",
       " 'early wanna go back sleeep',\n",
       " 'dont think sister refusn get ticket',\n",
       " 'oh wowhope he ok u take 2 vet',\n",
       " 'died wait magic jack read',\n",
       " 'ive woken',\n",
       " 'love',\n",
       " 'one greatest dinner ever',\n",
       " 'nooooo raining',\n",
       " 'shower class class taking care ladyfriend',\n",
       " 'great',\n",
       " 'great',\n",
       " 'goodmorning twitter oh gosh woke soooo nice lol oh hai thar twitterverse happy mothersday everybody',\n",
       " 'loved',\n",
       " 'hubby went pick fringe comic today store work store business comix houston 2 web',\n",
       " 'world amazing',\n",
       " 'completely lost',\n",
       " ' gadgetopia need dm youre following send email',\n",
       " 'cute',\n",
       " 'got cable set win got lock put door win feeling tad neglected fail',\n",
       " 'silly',\n",
       " 'great talk grace awesome god worksthat somehow started remote control work',\n",
       " 'beasted',\n",
       " 'ill listening',\n",
       " 'oh nou dont hit family emergency lil delay cant wait see',\n",
       " 'sitting home nothing gonna like weekend',\n",
       " 'unsalvageable',\n",
       " 'flew brisbane lax today great flight love light shame one drink limit thoug',\n",
       " 'think go sleep considering hav b 3hrs',\n",
       " 'told often wash hair would never speak',\n",
       " 'ended inventing',\n",
       " 'seems disappeared life',\n",
       " 'happy mother day mom every mom everywhere stroll beach laterhopefully',\n",
       " 'ohhh wanna go gahhh dunnooooooo confuzzzledd',\n",
       " 'haha',\n",
       " 'best',\n",
       " '',\n",
       " 'aww',\n",
       " 'nothing fair',\n",
       " 'going see long',\n",
       " 'better come time count 10 else 1 2 3 bam',\n",
       " 'prd take long time review',\n",
       " 'well',\n",
       " 'im going fail final',\n",
       " ' thx',\n",
       " 'sad saying im fat tear',\n",
       " 'thank might take offer',\n",
       " 'cant find anywhere',\n",
       " 'major headache',\n",
       " 'ch fu',\n",
       " 'mitchel idea much want call cost load call england',\n",
       " 'stupid',\n",
       " 'relaxing',\n",
       " 'yessir 100 right',\n",
       " 'night',\n",
       " 'great',\n",
       " 'sad',\n",
       " 'warm',\n",
       " 'ada acara menarik lain key vip di channel v ttg straight yg dikasih task approach stranger crowd hmm',\n",
       " 'well hit seeit depends',\n",
       " 'thanx',\n",
       " 'cute',\n",
       " 'good',\n",
       " 'shower time',\n",
       " 'coooooooool dooooooooown patience virtue',\n",
       " 'gotta drop car part buddy press dentist httpyfrogcom0a7v3j',\n",
       " 'cant call im work',\n",
       " 'expensive',\n",
       " 'tired ',\n",
       " 'happy',\n",
       " 'herefinally yay seriously made day oh fellow hell ruler',\n",
       " 'goodnight',\n",
       " 'doesnt hangover getting ready good ol english fry',\n",
       " 'waiting put story stereo skyline dont know put boooo',\n",
       " 'working',\n",
       " 'starbucks farrah',\n",
       " ' ginniejean there lot mojo place yes',\n",
       " 'third date went wellmoving fourth',\n",
       " 'mad tired hols miss chomp chomp terribly',\n",
       " 'finally starting assignment',\n",
       " 'good',\n",
       " 'good',\n",
       " 'glad',\n",
       " 'missing',\n",
       " 'rain',\n",
       " 'yeah went 5 day im trying get back november mom super soak star weekend',\n",
       " 'miss u guy prob wont b bk til august sumtime',\n",
       " 'okay make sure he alright kk cuidalo let know he got friend u aha',\n",
       " 'round way ahve copy public contact personal contact first big deal',\n",
       " 'trying figure',\n",
       " 'bob7 ill update leave vet test xrays',\n",
       " 'went see hannah montana movie saturday loved still cant work twitter though',\n",
       " 'im victim',\n",
       " 'would slip fall',\n",
       " 'looking forward',\n",
       " 'well paisley one cone thing around headso funny feel bad',\n",
       " 'stop',\n",
       " 'cant wait november jobros concert ',\n",
       " 'done always ended selling machine going back mac',\n",
       " 'feel sorry marcus though',\n",
       " 'sitting thru boring bit titanic waiting good bit start couple hour',\n",
       " 'httptwitpiccom4jken fire urban rock challenge',\n",
       " 'cant believe',\n",
       " 'sorry ba',\n",
       " 'well',\n",
       " '250e crazzyyyy summer cuz ci close rd',\n",
       " 'jus love doin night shift',\n",
       " 'morning tweepleway early',\n",
       " 'reupload thing',\n",
       " 'update really amusing',\n",
       " 'love fact',\n",
       " 'bad',\n",
       " 'awesome',\n",
       " 'rough',\n",
       " 'aww congrats family',\n",
       " 'yay',\n",
       " 'cant',\n",
       " 'strongly agrees jason wolverine hugh jackman sorry kid mouse',\n",
       " 'okay bby',\n",
       " 'goodnight',\n",
       " 'im late he gone',\n",
       " 'lost internet signal',\n",
       " 'sorry',\n",
       " 'good mate sadly couldnt get pissed tonight driving bad time',\n",
       " 'blood everywhere',\n",
       " 'pulled walmart aunt got went fell asleep 2 hour later r lol',\n",
       " 'searching home thing cook dinner evening mother day guess im eating',\n",
       " 'cute',\n",
       " 'woke 750 fell back sleep woke 850 back sleep woke 950 im staying awake morning',\n",
       " 'great',\n",
       " 'hurt arm',\n",
       " '',\n",
       " 'tiiiiiiired going bed',\n",
       " 'kind mood reason lol ill try mushy around behave',\n",
       " 'hope',\n",
       " 'keep getting delayed response internet messed',\n",
       " 'dont tell burnsy comparison rocky mountain mountain enlgland',\n",
       " 'sad',\n",
       " 'good',\n",
       " 'could get away dare ill go buy tomorrow',\n",
       " 'thanks man sorted',\n",
       " 'enjoying',\n",
       " 'favourite',\n",
       " 'ok back packing sitting car charging phone charger left home well',\n",
       " 'congratulatory',\n",
       " 'sadly thats often',\n",
       " 'good time',\n",
       " 'sun cool',\n",
       " ' meeeaaannn',\n",
       " 'invisible car help boost recycling honest',\n",
       " 'baby baby gonna right im side whole world turn ipod randoms bsb song',\n",
       " 'pretty',\n",
       " 'really love',\n",
       " 'sadly',\n",
       " 'kickable',\n",
       " 'also saw u american idol great performance see u windsor june 20 commodore wont',\n",
       " 'nice',\n",
       " 'happy',\n",
       " 'finished watching marley',\n",
       " 'enjoying',\n",
       " 'everyone seems love felt kinda lazy repetative really disappointed',\n",
       " 'roll thursday',\n",
       " 'dadgum think nation shipping need shut completely still freight carrier',\n",
       " 'cavs got lucky 2night lol lebron took killed em another triple double',\n",
       " 'happy',\n",
       " 'bad',\n",
       " 'forgot',\n",
       " 'long someone make',\n",
       " 'thanxx',\n",
       " 'arent following cant send dm',\n",
       " 'poorly',\n",
       " 'happy st',\n",
       " 'picked taco lunch got guitar',\n",
       " 'wont chance later',\n",
       " 'sold',\n",
       " 'hope',\n",
       " 'whoaa kinda hard one think interesting enough tell',\n",
       " 'youre best',\n",
       " 'listening 2 music home alone lol want 2 come hang lol',\n",
       " 'shut face',\n",
       " 'month put 2 week notice little week ago last day friday',\n",
       " 'oh yea mspacers boy hooked page look kewl least think took 5 minute wwwmyspacecommissmickey',\n",
       " 'cant wait',\n",
       " 'laavly',\n",
       " 'yes please youre gone might actually get work done lol',\n",
       " 'im really getting sick ugh',\n",
       " 'stalkerishly',\n",
       " ' youtube ',\n",
       " 'calling mum ask icecream',\n",
       " ' lazy day playing xbox drinking tea',\n",
       " 'disappointing',\n",
       " 'hopefully',\n",
       " 'mom texted told rodney chasing firefly backyard awwwww im miss',\n",
       " 'happy',\n",
       " 'thanks',\n",
       " 'leg killing know good pain',\n",
       " 'fun',\n",
       " 'h wish could go',\n",
       " 'ready go home',\n",
       " 'need push diet last level good last week lost 1 lb better gain',\n",
       " 'excited',\n",
       " 'back killing wont keep dropping lowhope got someone pick back tho lol',\n",
       " '',\n",
       " 'mad rain got',\n",
       " 'grandparent place',\n",
       " 'hi chile 9 deg c winter comingcant wait cali sun',\n",
       " 'dont feel good',\n",
       " 'born',\n",
       " 'congradts ur show even tho wasnt lol',\n",
       " 'damjust finished watching prison break final breakomg dont think ive cried hard showfinally understand finale',\n",
       " 'excuse',\n",
       " 'glad friday two class lazy afternoon bad isnt nice warm today',\n",
       " 'good',\n",
       " 'awesome',\n",
       " 'im afraid',\n",
       " 'im best day',\n",
       " 'aid16 goodnight',\n",
       " 'welcome',\n",
       " 'thanks',\n",
       " 'thanks',\n",
       " 'feeling homely freddie funpacked day',\n",
       " 'haha remember ',\n",
       " 'hee exam ill give u plenty show haha hopeless',\n",
       " 'sorry',\n",
       " 'awww remind getting ready ball',\n",
       " 'couldnt remember',\n",
       " 'bishop sez need get u look flight email ur work tonite',\n",
       " 'wish battery life iphone',\n",
       " 'happy',\n",
       " 'omg',\n",
       " 'better',\n",
       " 'work heat horrible ',\n",
       " 'turned human',\n",
       " 'glad',\n",
       " 'starr yep hawkesbury classic start windsor home said dj',\n",
       " 'thanks savvv',\n",
       " 'sleep pattern screwed',\n",
       " 'isnt easy find',\n",
       " 'may fourth happy star war day',\n",
       " 'lucky never',\n",
       " 'taking care yucky stuff',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pre_processed_text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "70aeef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(data['pre_processed_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bbe8adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2a85b",
   "metadata": {},
   "source": [
    "###### Impact on TF-IDF\n",
    "- While TF-IDF addresses some BoW limitations by giving more weight to important words, it still doesn't capture word relationships and semantics well.\n",
    "- OOV Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b170356",
   "metadata": {},
   "source": [
    "#### Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17e7e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Word2Vec(data['pre_processed_text'].values.tolist(), vector_size=100, window=5, min_count=2, sg=0)\n",
    "vocab = cbow.wv.index_to_key\n",
    "\n",
    "def get_mean_vector(model, sentence):\n",
    "    words = [word for word in sentence if word in vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model.wv[words], axis=0)\n",
    "    return np.zeros((100,))\n",
    "\n",
    "cbow_array = []\n",
    "for sentence in data['pre_processed_text'].values.tolist():\n",
    "    cbow_array.append(get_mean_vector(cbow, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1abd9214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27480, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_array = np.array(cbow_array)\n",
    "cbow_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44099ee",
   "metadata": {},
   "source": [
    "#### Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d77621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = Word2Vec(data['pre_processed_text'].values.tolist(), vector_size=100, window=5, min_count=2, sg=1)\n",
    "vocab = sg.wv.index_to_key\n",
    "\n",
    "def get_mean_vector(model, sentence):\n",
    "    words = [word for word in sentence if word in vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model.wv[words], axis=0)\n",
    "    return np.zeros((100,))\n",
    "\n",
    "sg_array = []\n",
    "for sentence in data['pre_processed_text'].values.tolist():\n",
    "    sg_array.append(get_mean_vector(sg, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b561c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27480, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_array = np.array(sg_array)\n",
    "sg_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b10d8",
   "metadata": {},
   "source": [
    "###### Impact on Word2Vec\n",
    "- These models are better at capturing semantic relationships and context, reducing misclassification related to word semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf470f6",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d21c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "data['sentiment'] = lb.fit_transform(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "254d6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ddbcc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bow, x_test_bow, y_train_bow, y_test_bow = train_test_split(count_matrix, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61c7ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_array, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c3fd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cbow, x_test_cbow, y_train_cbow, y_test_cbow = train_test_split(cbow_array, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aefebbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_skg, x_test_skg, y_train_skg, y_test_skg = train_test_split(sg_array, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18aa77ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) Shapes:\n",
      "x_train_bow shape: (21984, 17424)\n",
      "x_test_bow shape: (5496, 17424)\n",
      "y_train_bow shape: (21984,)\n",
      "y_test_bow shape: (5496,)\n",
      "=======================\n",
      "\n",
      "TF-IDF Shapes:\n",
      "x_train_tfidf shape: (21984, 17424)\n",
      "x_test_tfidf shape: (5496, 17424)\n",
      "y_train_tfidf shape: (21984,)\n",
      "y_test_tfidf shape: (5496,)\n",
      "=========================\n",
      "\n",
      "Continuous Bag of Words (CBOW) Shapes:\n",
      "x_train_cbow shape: (21984, 100)\n",
      "x_test_cbow shape: (5496, 100)\n",
      "y_train_cbow shape: (21984,)\n",
      "y_test_cbow shape: (5496,)\n",
      "========================\n",
      "\n",
      "Skip-Gram Shapes:\n",
      "x_train_skg shape: (21984, 100)\n",
      "x_test_skg shape: (5496, 100)\n",
      "y_train_skg shape: (21984,)\n",
      "y_test_skg shape: (5496,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Bag of Words (BoW) Shapes:\")\n",
    "print(\"x_train_bow shape:\", x_train_bow.shape)\n",
    "print(\"x_test_bow shape:\", x_test_bow.shape)\n",
    "print(\"y_train_bow shape:\", y_train_bow.shape)\n",
    "print(\"y_test_bow shape:\", y_test_bow.shape)\n",
    "print(\"=======================\")\n",
    "print(\"\\nTF-IDF Shapes:\")\n",
    "print(\"x_train_tfidf shape:\", x_train_tfidf.shape)\n",
    "print(\"x_test_tfidf shape:\", x_test_tfidf.shape)\n",
    "print(\"y_train_tfidf shape:\", y_train_tfidf.shape)\n",
    "print(\"y_test_tfidf shape:\", y_test_tfidf.shape)\n",
    "print(\"=========================\")\n",
    "print(\"\\nContinuous Bag of Words (CBOW) Shapes:\")\n",
    "print(\"x_train_cbow shape:\", x_train_cbow.shape)\n",
    "print(\"x_test_cbow shape:\", x_test_cbow.shape)\n",
    "print(\"y_train_cbow shape:\", y_train_cbow.shape)\n",
    "print(\"y_test_cbow shape:\", y_test_cbow.shape)\n",
    "print(\"========================\")\n",
    "print(\"\\nSkip-Gram Shapes:\")\n",
    "print(\"x_train_skg shape:\", x_train_skg.shape)\n",
    "print(\"x_test_skg shape:\", x_test_skg.shape)\n",
    "print(\"y_train_skg shape:\", y_train_skg.shape)\n",
    "print(\"y_test_skg shape:\", y_test_skg.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294bd3f",
   "metadata": {},
   "source": [
    "### Model Builing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "066ad619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_decision_tree(x_train, x_test, y_train, y_test, representation):\n",
    "    \n",
    "    dtclassifier = DecisionTreeClassifier(random_state=9,max_depth=5)\n",
    "    dtclassifier.fit(x_train, y_train)\n",
    "    y_pred = dtclassifier.predict(x_test)\n",
    "\n",
    "    print(f\"\\nMetrics for {representation}:\")\n",
    "    print(f\"Model Score: {dtclassifier.score(x_train,y_train)}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b19be50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_navie_bayes(x_train, x_test, y_train, y_test, representation):\n",
    "    \n",
    "    nbclassifier = MultinomialNB()\n",
    "    nbclassifier.fit(x_train, y_train)\n",
    "    y_pred = nbclassifier.predict(x_test)\n",
    "\n",
    "    print(f\"\\nMetrics for {representation}:\")\n",
    "    print(f\"Model Score: {nbclassifier.score(x_train,y_train)}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    return nbclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8637bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for BoW:\n",
      "Model Score: 0.4932678311499272\n",
      "Accuracy: 0.4798034934497817\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.00      0.00      1548\n",
      "           1       0.43      0.94      0.59      2182\n",
      "           2       0.79      0.33      0.46      1766\n",
      "\n",
      "    accuracy                           0.48      5496\n",
      "   macro avg       0.61      0.42      0.35      5496\n",
      "weighted avg       0.59      0.48      0.39      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_decision_tree(x_train_bow, x_test_bow, y_train_bow, y_test_bow, \"BoW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5be64ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for TF-IDF:\n",
      "Model Score: 0.5020469432314411\n",
      "Accuracy: 0.4885371179039301\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1548\n",
      "           1       0.44      0.99      0.61      2182\n",
      "           2       0.91      0.30      0.45      1766\n",
      "\n",
      "    accuracy                           0.49      5496\n",
      "   macro avg       0.78      0.43      0.35      5496\n",
      "weighted avg       0.75      0.49      0.39      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_decision_tree(x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf, \"TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35045729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for CBOW:\n",
      "Model Score: 0.6676219068413392\n",
      "Accuracy: 0.6519286754002911\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.52      0.56      1548\n",
      "           1       0.67      0.80      0.73      2182\n",
      "           2       0.66      0.58      0.62      1766\n",
      "\n",
      "    accuracy                           0.65      5496\n",
      "   macro avg       0.65      0.63      0.64      5496\n",
      "weighted avg       0.65      0.65      0.65      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_decision_tree(x_train_cbow, x_test_cbow, y_train_cbow, y_test_cbow, \"CBOW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77a8027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Skip-Gram:\n",
      "Model Score: 0.6605258369723436\n",
      "Accuracy: 0.6486535662299855\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.55      0.58      1548\n",
      "           1       0.62      0.83      0.71      2182\n",
      "           2       0.78      0.51      0.62      1766\n",
      "\n",
      "    accuracy                           0.65      5496\n",
      "   macro avg       0.67      0.63      0.63      5496\n",
      "weighted avg       0.67      0.65      0.64      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_decision_tree(x_train_skg, x_test_skg, y_train_skg, y_test_skg, \"Skip-Gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c8bf9c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for BoW:\n",
      "Model Score: 0.8563045851528385\n",
      "Accuracy: 0.7530931586608443\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.61      0.69      1548\n",
      "           1       0.71      0.81      0.76      2182\n",
      "           2       0.78      0.80      0.79      1766\n",
      "\n",
      "    accuracy                           0.75      5496\n",
      "   macro avg       0.76      0.74      0.75      5496\n",
      "weighted avg       0.76      0.75      0.75      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nbc_1 = train_and_evaluate_navie_bayes(x_train_bow, x_test_bow, y_train_bow, y_test_bow, \"BoW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dee8364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Tf-IDF:\n",
      "Model Score: 0.8605349344978166\n",
      "Accuracy: 0.774745269286754\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.59      0.70      1548\n",
      "           1       0.71      0.89      0.79      2182\n",
      "           2       0.82      0.79      0.81      1766\n",
      "\n",
      "    accuracy                           0.77      5496\n",
      "   macro avg       0.80      0.76      0.77      5496\n",
      "weighted avg       0.79      0.77      0.77      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nbc_2 = train_and_evaluate_navie_bayes(x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf, \"Tf-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970fc06",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6548aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"What is not to like about this product.\",\n",
    "    \"Not bad.\",\n",
    "    \"Not an issue.\",\n",
    "    \"Not buggy.\",\n",
    "    \"Not happy.\",\n",
    "    \"Not user-friendly.\",\n",
    "    \"Not good.\",\n",
    "    \"Is it any good?\",\n",
    "    \"I do not dislike horror movies.\",\n",
    "    \"Disliking horror movies is not uncommon.\",\n",
    "    \"Sometimes I really hate the show.\",\n",
    "    \"I love having to wait two months for the next series to come out!\",\n",
    "    \"The final episode was surprising with a terrible twist at the end.\",\n",
    "    \"The film was easy to watch but I would not recommend it to my friends.\",\n",
    "    \"I LOL’d at the end of the cake scene\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "94e9d78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is not to like about this product.: Positive\n",
      "Not bad.: Negative\n",
      "Not an issue.: Negative\n",
      "Not buggy.: Neutral\n",
      "Not happy.: Positive\n",
      "Not user-friendly.: Positive\n",
      "Not good.: Positive\n",
      "Is it any good?: Positive\n",
      "I do not dislike horror movies.: Negative\n",
      "Disliking horror movies is not uncommon.: Negative\n",
      "Sometimes I really hate the show.: Neutral\n",
      "I love having to wait two months for the next series to come out!: Neutral\n",
      "The final episode was surprising with a terrible twist at the end.: Neutral\n",
      "The film was easy to watch but I would not recommend it to my friends.: Neutral\n",
      "I LOL’d at the end of the cake scene: Neutral\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    preprocessed_text = \" \".join(simple_preprocess(text))\n",
    "    transformed_text = tfidf.transform([preprocessed_text]).toarray()\n",
    "    prediction = nbc_1.predict(transformed_text)[0]\n",
    "    \n",
    "    if prediction == 0:\n",
    "        print(f\"{text}: Negative\")\n",
    "    elif prediction == 1:\n",
    "        print(f\"{text}: Neutral\")\n",
    "    elif prediction == 2:\n",
    "        print(f\"{text}: Positive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
